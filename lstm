import numpy as np
import pandas as pd
import random
import math as m

import sys
import os
import csv

from cntk.device import *
from cntk import Trainer
from cntk.layers import * 
from cntk.layers.typing import *
from cntk.learners import *
from cntk.ops import *
from cntk.logging import *
from cntk.metrics import *
from cntk.losses import *
from cntk.io import *

import cntk
import cntk.ops as o
import cntk.layers as l


from _cntk_py import set_fixed_random_seed, force_deterministic_algorithms

input_dim = 70
output_dim = 56
lstm_cell_dimension = 20

train_file_path = 'vvv'
test_file_path = 'xx'
LSTM_USE_PEEPHOLES=True
BIAS=False

set_default_device(cpu())
set_fixed_random_seed(1)


def create_train_data():

	listOfTuplesOfInputsLabels = []
	listOfInputs = []
	listOfLabels = []
	
	listOfTestInputs = []

	train_df = pd.read_csv(train_file_path , sep="|", header = None)
	
	for index, row in train_df.iterrows():
		input_df = train_df.values[index, 1]
		output_df = train_df.values[index, 2]

		input_split_df = input_df.split()
		output_df_df = output_df.split()

		input_array_df = np.asarray(input_split_df[1:len(input_split_df)], dtype=np.float32)
		output_array_df = np.asarray(output_df_df[1:len(output_df_df)], dtype=np.float32)

		tup=(input_array_df, output_array_df)
		listOfTuplesOfInputsLabels.append(tup)

	random.shuffle(listOfTuplesOfInputsLabels)
	
	for iseries in range(len(listOfTuplesOfInputsLabels)):
		series=listOfTuplesOfInputsLabels[iseries]
		listOfInputs.append(series[0])
		listOfLabels.append(series[1])
	
	listOfInputs= np.asarray(listOfInputs,dtype=np.float32)
	listOfLabels= np.asarray(listOfLabels,dtype=np.float32)
	
	test_df = pd.read_csv(test_file_path , sep="|", header = None)
	
	for index, row in test_df.iterrows():
		input_test_df = test_df.values[index, 1]
		input_split_test_df = input_test_df.split()

		input_test_array_df = np.asarray(input_split_test_df[1:len(input_split_test_df)], dtype=np.float32)
		listOfTestInputs.append(input_test_array_df)
	
	return listOfInputs, listOfLabels, listOfTestInputs
	
 def train_model(features, labels,tests):

	gaussian_noise = 0.0004
	l2_regularization_weight = 0.0005
	minibatch_size = 128
	#test_minitbacth_size = 1
	max_epochs =10
	
	
	num_minibatches = len(features) // minibatch_size
	epoch_size = len(features)*1
	
	feature = o.input_variable((input_dim),np.float32)
	label = o.input_variable((output_dim),np.float32)
	
	#Run the trainer on and perform model training
	num_passes = 1
	
	netout=Sequential([For(range(1), lambda i: Recurrence(LSTM(lstm_cell_dimension,use_peepholes=LSTM_USE_PEEPHOLES,init=glorot_uniform(seed=set_fixed_random_seed(1))))),
                         Dense(output_dim,bias=BIAS,init=glorot_uniform(seed=set_fixed_random_seed(1)))])(feature)
	
	ce = squared_error(netout,label)
	pe = squared_error(netout, label)
	
	
	learner = momentum_sgd(netout.parameters, lr = learning_rate_schedule([(4,0.003),(16,0.002)], unit=UnitType.sample,epoch_size=epoch_size),
                           momentum=momentum_as_time_constant_schedule(minibatch_size / -m.log(0.9)), gaussian_noise_injection_std_dev = gaussian_noise,l2_regularization_weight =l2_regularization_weight)
	progress_printer = ProgressPrinter(1)
	trainer = Trainer(netout,(ce, pe), learner, progress_printer)
	
	tf = np.array_split(features,num_minibatches)
	tl = np.array_split(labels,num_minibatches)
	
	
	for epoch in range(max_epochs):	# loop over epochs
		for i in range(num_minibatches*num_passes): # multiply by the 
			features = np.ascontiguousarray(tf[i%num_minibatches])
			labels = np.ascontiguousarray(tl[i%num_minibatches])
			features= np.array(features).tolist()
			labels = np.array(labels).tolist()
			# Specify the mapping of input variables in the model to actual minibatch data to be trained with
			trainer.train_minibatch({feature : features, label : labels})
			progress_printer.update_with_trainer(trainer, with_metric=True) # log progress
		loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)
		print(learner.learning_rate())
		
	test_output=trainer.model.eval({feature: tests})
	
	return test_output
	
if __name__ == '__main__':

	set_default_device(cpu())
	
	np.random.seed(set_fixed_random_seed(1))
	random.seed(set_fixed_random_seed(1))
	
	set_fixed_random_seed(1)
	force_deterministic_algorithms()

	#force_deterministic_algorithms(true)

	listOfTestInput = []
    # Specify the target device to be used for computing, if you do not want to
    # use the best available one, e.g.
    # try_set_default_device(cpu())
	features, labels,tests = create_train_data()
	testshape = train_model(features, labels,tests)
	
	#print(shape[0])
	for il in range(len(testshape)):
		oneTestOut=testshape[il]
		listvalue = oneTestOut[oneTestOut.shape[0]-1,]
		listvalue= np.array(listvalue).tolist()
		listOfTestInput.append(listvalue)
	
	with open("forecastingcluster2.txt", "w") as output:
		writer = csv.writer(output, lineterminator='\n')
		writer.writerows(listOfTestInput)
